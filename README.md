Final Year Project (Team of 4)

ABSTRACT

Our project introduces a user-friendly approach to image inpainting, leveraging text descriptions for intuitive editing. Inpainting replaces or edits specific areas of an image, which relies on a mask to determine which regions of an image to 
fill in. This project explores a novel approach to text-guided image inpainting, empowering users to control the content filling these masked areas through text descriptions. Our method leverages recent advancements in deep learning, particularly by combining our modelâ€™s adept at understanding the relationships between text and images with powerful diffusion models capable of generating realistic image content. This approach differentiates itself from existing
techniques by enabling a more user-centric and semantically aware inpainting
process. Users provide a source image with a masked region and a text prompt
describing the desired content. The model then seamlessly integrates user-defined
content into the image, ensuring it aligns with the provided description. This
research delves into understanding the mechanisms of diffusion models within
this context. We explore methods for further enhancing the realism of generated
outputs, aiming to contribute to the development of even more powerful and userfriendly
tools.
